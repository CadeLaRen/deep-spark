/*
 * Copyright 2014, Stratio.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.stratio.deep.extractor.impl;

import static com.stratio.deep.utils.Constants.SPARK_PARTITION_ID;
import static com.stratio.deep.utils.Constants.SPARK_RDD_ID;
import com.stratio.deep.config.ExtractorConfig;
import com.stratio.deep.config.IDeepJobConfig;
import com.stratio.deep.entity.Cells;
import com.stratio.deep.functions.AbstractSerializableFunction;
import com.stratio.deep.rdd.IExtractor;
import com.stratio.deep.utils.DeepSparkHadoopMapReduceUtil;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.*;
import org.apache.spark.Partition;
import org.apache.spark.rdd.NewHadoopPartition;
import org.apache.spark.rdd.RDD;
import scala.Tuple2;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.List;

/**
 * Created by rcrespo on 26/08/14.
 */
public abstract class GenericHadoopExtractor<T, K, V> implements IExtractor<T> {


    protected IDeepJobConfig<T, ? extends IDeepJobConfig> deepJobConfig;

    protected transient RecordReader<K, V> reader;

    protected transient RecordWriter<K, V> writer;

    protected transient InputFormat<K,V> inputFormat;

    protected transient OutputFormat<K,V> outputFormat;

    protected transient String jobTrackerId;

    protected transient TaskAttemptContext hadoopAttemptContext;

    protected boolean havePair = false;

    protected boolean finished = false;

    protected transient JobID jobId = null;

    protected AbstractSerializableFunction<T, Tuple2<K, V>> transformFunction;

    {
        SimpleDateFormat formatter = new SimpleDateFormat("yyyyMMddHHmm");
        jobTrackerId = formatter.format(new Date());

    }




    @Override
    public Partition[] getPartitions(ExtractorConfig<T> config) {

        int id = Integer.parseInt(config.getValues().get(SPARK_RDD_ID));
        jobId = new JobID(jobTrackerId, id);

        deepJobConfig = (IDeepJobConfig<T, ? extends IDeepJobConfig>) deepJobConfig.initialize(config);
        Configuration conf = deepJobConfig.getHadoopConfiguration();


        JobContext jobContext = DeepSparkHadoopMapReduceUtil.newJobContext(conf, jobId);


        try {
            List<InputSplit> splits = inputFormat.getSplits(jobContext);


            Partition[] partitions = new Partition[(splits.size())];
            for (int i = 0 ; i < splits.size(); i++) {
                partitions[i] = new NewHadoopPartition(id, i, splits.get(i));
            }

            return partitions;

            //TODO autogenerated
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }


        return null;
    }

    @Override
    public boolean hasNext() {
        if (!finished && !havePair) {
            try {
                finished = !reader.nextKeyValue();
            } catch (IOException | InterruptedException e) {
                e.printStackTrace();
            }
            havePair = !finished;

        }
        return !finished;
    }

    @Override
    public T next() {
        if (!hasNext()) {
            throw new java.util.NoSuchElementException("End of stream");
        }
        havePair = false;

        Tuple2<K, V> tuple = null;
        try {
            tuple = new Tuple2<>((K)reader.getCurrentKey(), (V)reader.getCurrentValue());
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
        return transformElement(tuple, deepJobConfig);
    }

    @Override
    public void close() {
        try {
            if(reader!=null){
                reader.close();
            }
            if(writer!=null){
                writer.close(hadoopAttemptContext);
            }
            //TODO AUTOGENERATED
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void initIterator(Partition dp, ExtractorConfig<T> config) {

        deepJobConfig.initialize(config);

        int id = Integer.parseInt(config.getValues().get(SPARK_RDD_ID));
        NewHadoopPartition split = (NewHadoopPartition)dp;

        TaskAttemptID attemptId = DeepSparkHadoopMapReduceUtil.newTaskAttemptID(jobTrackerId, id, true, split.index(), 0);


        TaskAttemptContext hadoopAttemptContext = DeepSparkHadoopMapReduceUtil.newTaskAttemptContext(deepJobConfig.getHadoopConfiguration(), attemptId);


        try {
            reader = inputFormat.createRecordReader(split.serializableHadoopSplit().value(), hadoopAttemptContext);
            reader.initialize(split.serializableHadoopSplit().value(), hadoopAttemptContext);
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }

    }

    public abstract T transformElement(Tuple2<K, V> tuple, IDeepJobConfig<T, ? extends IDeepJobConfig> config);

    @Override
    public IExtractor getExtractorInstance(ExtractorConfig<T> config) {
        return this;
    }


    @Override
    public  void saveRDD(T t){
        Tuple2<K, V> tuple = transformElement(t);
        try {
            writer.write(tuple._1(),tuple._2());

            //TODO AUTOGENERATED
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
        return;
    }

    @Override
    public void initSave(ExtractorConfig<T> config, T first){
        int id = Integer.parseInt(config.getValues().get(SPARK_RDD_ID));

        int partitionIndex = Integer.parseInt(config.getValues().get(SPARK_PARTITION_ID));

        TaskAttemptID attemptId = DeepSparkHadoopMapReduceUtil.newTaskAttemptID(jobTrackerId, id, true, partitionIndex, 0);

        hadoopAttemptContext = DeepSparkHadoopMapReduceUtil.newTaskAttemptContext(deepJobConfig.initialize(config).getHadoopConfiguration(), attemptId);
        try {
            writer = outputFormat.getRecordWriter(hadoopAttemptContext);
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }
    }

    public abstract Tuple2<K, V>  transformElement(T record);
}
