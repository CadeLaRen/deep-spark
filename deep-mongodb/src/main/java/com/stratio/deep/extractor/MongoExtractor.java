package com.stratio.deep.extractor;

import com.mongodb.hadoop.MongoInputFormat;
import com.mongodb.hadoop.input.MongoRecordReader;
import com.stratio.deep.config.ExtractorConfig;
import com.stratio.deep.config.IMongoDeepJobConfig;
import com.stratio.deep.rdd.DeepTokenRange;
import com.stratio.deep.rdd.IExtractor;
import com.stratio.deep.utils.DeepSparkHadoopMapReduceUtil;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.*;
import org.apache.spark.Partition;
import org.apache.spark.rdd.NewHadoopPartition;
import org.apache.spark.rdd.RDD;
import org.bson.BSONObject;
import scala.Tuple2;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.List;

/**
 * Created by rcrespo on 21/08/14.
 */
public abstract class MongoExtractor<T> implements IExtractor<T> {


    protected IMongoDeepJobConfig<T> mongoJobConfig;

    private transient MongoRecordReader reader;

    private transient MongoInputFormat inputFormat = new MongoInputFormat();


    private transient String jobTrackerId;

    boolean havePair = false;

    boolean finished = false;

    protected transient JobID jobId = null;

    {
        SimpleDateFormat formatter = new SimpleDateFormat("yyyyMMddHHmm");
        jobTrackerId = formatter.format(new Date());

    }

//    @transient protected JobID jobId = new JobID(jobTrackerId, id)

    //TODO initialize


    @Override
    public Partition[] getPartitions(ExtractorConfig<T> config) {

        int id = Integer.parseInt(config.getValues().get("spark.rdd.id"));
        jobId = new JobID(jobTrackerId, id);

        mongoJobConfig = mongoJobConfig.initialize(config);
        Configuration conf = mongoJobConfig.getHadoopConfiguration();


        JobContext jobContext = DeepSparkHadoopMapReduceUtil.newJobContext(conf, jobId);


        try {
            List<InputSplit> splits = inputFormat.getSplits(jobContext);

            DeepTokenRange[] tokens = new DeepTokenRange[splits.size()];
            for (int i = 0; i < splits.size(); i++) {
                tokens[i] = new DeepTokenRange(splits.get(i).getLocations());
            }


            Partition[] partitions = new Partition[(splits.size())];
            for (int i = 0; i < splits.size(); i++) {
                partitions[i] = new NewHadoopPartition(id, i, splits.get(i));
            }

            return partitions;

            //TODO autogenerated
        } catch (IOException | InterruptedException e) {
            e.printStackTrace();
        }


        return null;
    }

    @Override
    public boolean hasNext() {
        if (!finished && !havePair) {
            finished = !reader.nextKeyValue();
            havePair = !finished;

        }
        return !finished;
    }

    @Override
    public T next() {
        if (!hasNext()) {
            throw new java.util.NoSuchElementException("End of stream");
        }
        havePair = false;

        Tuple2<Object, BSONObject> tuple = new Tuple2<>(reader.getCurrentKey(), reader.getCurrentValue());
        return transformElement(tuple, mongoJobConfig);
    }

    @Override
    public void close() {
        reader.close();
    }

    @Override
    public void initIterator(Partition dp, ExtractorConfig<T> config) {

        mongoJobConfig = mongoJobConfig.initialize(config);

        int id = Integer.parseInt(config.getValues().get("spark.rdd.id"));
        NewHadoopPartition split = (NewHadoopPartition) dp;

        TaskAttemptID attemptId = DeepSparkHadoopMapReduceUtil.newTaskAttemptID(jobTrackerId, id, true, split.index(), 0);


        TaskAttemptContext hadoopAttemptContext = DeepSparkHadoopMapReduceUtil.newTaskAttemptContext(mongoJobConfig.getHadoopConfiguration(), attemptId);


        reader = (MongoRecordReader) inputFormat.createRecordReader(split.serializableHadoopSplit().value(), hadoopAttemptContext);
        reader.initialize(split.serializableHadoopSplit().value(), hadoopAttemptContext);

    }

    public abstract T transformElement(Tuple2<Object, BSONObject> tuple, IMongoDeepJobConfig<T> config);


    public abstract void saveRDD(RDD<T> rdd, ExtractorConfig<T> config);
}
